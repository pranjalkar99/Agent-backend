{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "resp = OpenAI(model=\"gpt-4\", api_key=\"zu-0418787c5f8c49648ed1c4edf1ddb501\", api_base=\"https://zukijourney.xyzbot.net/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='The capital of France is Paris. Paris is one of the most famous cities in the world, known for its rich history, art, fashion, and cuisine. It is home to many famous landmarks, including the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Palace of Versailles. Paris is located in the north-central part of France and is the largest city in the country, with a population of over 2 million people. It is also the capital of the Ile-de-France region and is an important cultural, political, and economic center in Europe.', additional_kwargs={}, raw={'id': 'chatcmpl-AknimuPhzIgylAP6Xej3MafSYbjx', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris. Paris is one of the most famous cities in the world, known for its rich history, art, fashion, and cuisine. It is home to many famous landmarks, including the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Palace of Versailles. Paris is located in the north-central part of France and is the largest city in the country, with a population of over 2 million people. It is also the capital of the Ile-de-France region and is an important cultural, political, and economic center in Europe.', role='assistant', function_call=None, tool_calls=None))], 'created': 1713105209, 'model': 'gpt-4', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=134, prompt_tokens=8, total_tokens=142)}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.complete(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(\n",
    "    ytlinks=[\"https://www.youtube.com/watch?v=i3OYlaoj-BM\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='i3OYlaoj-BM', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"the imaginative laws scale visual\\nrecognition challenge was a\\nworld-changing competition\\nthat ran from around 2010 to 2017.\\nduring his time the competition acted as\\nthe place to go if you needed to find\\nwhat the current state of the art was in\\nimage classification object localization\\nobject detection as well as that\\n2012 onwards it really acted as the\\nCatalyst of the explosion in deep\\nlearning researchers fine-tuned better\\nperforming computer vision models year\\non year but there was a unquestioned\\nassumption causing problems\\nit was assumed that every new task\\nrequired model fine tuning this required\\na lot of data and a lot of data required\\na lot of capital and time it wasn't\\nuntil recently that this assumption was\\nchallenged and proven wrong the\\nastonishing rise of what are called\\nmulti-modal models has made the what was\\nthought impossible very possible across\\nvarious domains and tasks one of those\\nis called zero shot object detection and\\nlocalization now zero shot refers to\\ntaking a model and applying it to a new\\ndomain without ever fine-tuning it on\\ndata from that new domain so that means\\nwe can take a model who can it maybe it\\nworks in in one domain classification in\\none particular area on one data set and\\nwe can take that same model without any\\nfine tuning and we can use it for object\\ndetection in a completely different\\ndomain without that model seeing any\\ntraining data from that new domain so in\\nthis video we're going to explore how to\\nuse open ai's clip for zero shots object\\ndetection and localization let's begin\\nwith taking a quick look at image\\nclassification now image classification\\ncan kind of be seen as one of the\\nsimplest tasks in visual recognition and\\nit's also the first step on the way to\\nobject detection at his core it's just\\nassigning a categorical label to an\\nimage now moving on from image\\nclassification we have object\\nlocalization object localization is\\nimage classification followed by the\\nidentification of where in the image the\\nspecific object actually is so we are\\nlocalizing the the object now doing that\\nwhere essentially just going to identify\\nthe coordinates on the image I going to\\nreturn the typical approach to this is\\nreturn an image where you have like a\\nbounding box surrounding the the object\\nthat you are looking for and then we\\ntake this one step further to perform\\nobject detection with detection we are\\nlocalizing multiple objects within the\\nimage or we have the capability to\\nidentify multiple objects within the\\nimage so in this example we have cat and\\na dog we would expect with object\\ndetection to identify both the cat and\\nthe dot in the case of us having\\nmultiple dolbs in this image almost Cuts\\nin this image we would also expect the\\nobject detection algorithm to actually\\nidentify each one of those independently\\nnow in the past if we wanted to switch a\\nmodel between anyone these tasks would\\nhave to fine-tune or more data you want\\nto switch it to another domain we would\\nhave to also fine tune it on new data\\nfrom that domain but that's not always\\nthe case with models like open ai's clip\\nfor performing each one of these tasks\\nin a zero shot setting now open ai's\\nclip is a multi-modal model that has\\nbeen pre-trained on a huge number of\\ntext and image Pairs and it essentially\\nworks by identifying text and image\\npairs that have a similar meaning and\\nplacing them within a similar Vector\\nspace so every text and every image gets\\nconverted into a vector and they are\\nplaced in a shared Vector space and the\\nvectors that appear close together they\\nhave a similar meaning now Clips very\\nbroad pre-training means that it can\\nperform very effectively across a lot of\\ndifferent domains it's seen a lot of\\ndata and so it has a good understanding\\nof all these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we don't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks to Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our plus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with respect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then that is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we apply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire image into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of those patches moving\\nacross the entire image left to right\\ntop to bottom and we generate a image\\nembedding for each of those windows and\\nthen we calculate the similarity between\\neach one of those windows embedded by\\nclip and the class label embedding\\nreturning a similarity score for every\\nsingle patch now after calculating the\\nsimilarity score for every single patch\\nwe use that to create almost like a map\\nof relevance across the entire image and\\nthen we can use that map to identify the\\nlocation of the object of interest and\\nfrom that we will get something that's\\nkind of like this so we have most of the\\nimage will be very dark and black that\\nmeans as the object of interest is not\\nin that space and then using that\\nlocalization map we can create a more\\ntraditional bounding box visualization\\nas well both of these visuals are\\ncapturing the same information we're\\njust displaying it in a different way\\nnow there's also other approaches to\\nthis so I recently hosted a talk with\\nwhat two two sets of people actually so\\nFederico Bianchi from Stanford's NLP\\ngroup and also Raphael pissoni and both\\nof those have worked on a Italian clip\\nproject and part of that was performing\\nobject localization now to do that they\\nuse a slightly different approach what\\nI'm going to demonstrate here and we can\\nthink of it as almost like the opposite\\nso whereas we slide a window over the\\nwhole image they slide a black patch\\nover the whole image which hides what is\\nbehind in that patch and then they feed\\nthe image into click and essentially as\\nyou slide the patch over the image you\\nare hiding a part of the image and\\ntherefore if this similarity score drops\\nwhen the patch is over a certain area\\nyou know that the object you're looking\\nfor is probably within that space and\\nthat's called the occlusion algorithm\\nnow moving on to object detection which\\nis like the last level in these three\\ntasks we will be identifying multiple\\nobjects now there's a very fine line\\nbetween object localization and object\\ndetection but you can simply think of it\\nas localization for multiple clusters\\nand multiple objects with our cat and\\nButterfly image we will be searching for\\ntwo objects a cat and a butterfly and\\nwith that we could draw a bounding box\\naround both of those objects and\\nessentially what we're doing now is\\nusing localization for a single object\\nbut then we're putting both of those\\ntogether in a loop in our code and we're\\nproducing this object detection process\\nnow we've covered the idea behind my\\nimage classification onto object\\nlocalization and object detection now\\nlet's have a look at how we actually\\nImplement all of this now before we move\\non to any classification localization or\\ndetection task we need to have some data\\nwe're going to use a small demo data set\\ncalled James Callum image text demo and\\nwe can download it like this so we're\\nusing hooking phase data sets here which\\nwe can pip install with Pip install\\ndata sets\\nand this is the day so it's very small\\nit's 21 text to image pairs okay one of\\nthose is the image you've already seen\\nthe cat with a butterfly landing on its\\nnose very curious how they got that\\nphoto now after you've downloaded that\\ndata set\\nwe're going to be using this image here\\nand what we want to do is not use the\\nimage file itself because at the moment\\nit's a it's a pill python image object\\nbut instead we need to convert it into a\\ncanister now we're going to be using pi\\ntorch later on so what I want to do here\\nis we're going to just transform the\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer vision and we just use tube\\ntensor okay and then we process our\\nimage through that Pipeline and then we\\ncan see that we get this okay so what\\nare these values here we have the height\\nof the image in pixels\\nthe width of the image in pixels and\\nthen also the three color channels red\\ngreen and blue that make up the image\\nnow we need a slightly different format\\nwhen we are processing everything one we\\nneed to add those patches and two we\\nneed to process it through a pie torch\\nmodel and we also need the batch\\ndimension for that so the first thing\\nwe're going to do is add the batch\\nDimension it's just a single image so we\\njust have one in there but we we need\\nthat anyway and then we come down to\\nhere so this is where we're going to\\nbreak the image into the patches okay\\neach patch is going to be 256 dimensions\\nin both height and width so the first\\nthing we do here is unfold and we get\\nthis here okay there's two five six and\\nthere's 20. now the 20 is the height of\\nthe image in these 256 pixel patches\\nand we can visualize that here\\nall right so now we have all these kind\\nof like slivers of the image that's just\\na vertical component of each patch\\nand we use unfold again but in this time\\nin the second Dimensions the targeting\\nwhat was this Dimension here and we also\\nget another 256 now we visualize that we\\nget our four patches\\nokay like this\\nnow if you just consider this here it's\\nlike if we look at this patch here it\\ndoesn't tell us anything about the image\\nright and even when we're over cats\\nthese patches are way too small to\\nactually tell us anything if clip is\\nprocessing a single patch at a time\\nit's probably not going to tell us\\nanything maybe it could tell us that\\nthere's some hair in this patch or that\\nthere's an eye in this patch but beyond\\nthat it's not going to be very useful so\\nrather than feeding single patches into\\nclip what we do is actually feed a\\nwindow a six by six patches or we can\\nmodify that value if we prefer and that\\njust gives us a big patch to pass over\\nto clip now the reason that we don't\\njust do that from the start we don't\\njust create these bigger patches to\\nbegin with is because when we're sliding\\nthrough the image we want to have some\\ndegree of overlap between each patch\\nokay so we create these smaller patches\\nand then what we can do is actually\\nslide across just one little patch at a\\ntime and we Define that using the stride\\nvariable so if we come down to here\\nwe have window we have stride\\nremove this\\nand here we go this is our code for\\ngoing through the whole image creating a\\npatch at every time step okay so we go\\nfor y and we go through the whole y-axis\\nand then within that we're going across\\nleft to right with each step and we\\ninitialize a empty big patch array so\\nthis is our like the full window\\nwe got the current batch so okay let's\\nsay we start at zero zero X zero y zero\\nwe go from zero to six and zero to six\\nhere\\nright so that gives us the very top left\\ncorner or window of the image and then\\nwe're literally going through and and\\njust go processing all of that and you\\ncan see that happening here as wine eggs\\nare increasing we're moving through that\\nimage and we're seeing each big patch\\nfrom our image okay sliding across with\\na single small little patch at a time so\\nthat we don't miss any important\\ninformation\\nnow this is how we're going to run\\nthrough the whole image but before we do\\nthat we actually need clip so let's go\\nahead and actually initialize clip\\nso to do that all we do is this so we're\\nusing hook and face Transformers which\\nis using pi torch in the in the back\\nthere so we need the clip processor\\nwhich is like a pre-processing pipeline\\nfor both text and images and then the\\nactual model itself okay so we some\\nmodel ID and we initialize both of those\\nthen what we want to do is move the\\nmodel to advice if possible all right so\\nwe can use CPU but if you have a Kudo\\nenabled GPU that will be here much\\nfaster so I'd recommend doing that if\\nyou can if not then you can use CPU it\\nwill be a bit slower but it will still\\nrun within a variable time frame so if\\nI'm running this on my Mac I am using a\\nCPU you can actually run this on MPS as\\nwell so you could change your device to\\nMPS if you have an MPS enabled Apple\\nsilicon device\\nso now returning to that process where\\nwe're going through each window within\\nthe image we're just going to add a\\nlittle bit more logic so we are\\nprocessing like we were before there's\\nnothing different here we're creating\\nthat big patch and then what we do is\\nprocess that big patch and process a\\ntext label okay so at the moment we're\\nlooking for a fluffy cat within this\\nimage so that is how we do this we're\\nreturning Pi torch tensors we also add\\npadding here as well for the text\\nalthough in this case I don't think we\\nneed it because we only have a single\\ntext item but we include that when we're\\nusing multiple text items later and then\\nwe calculate and retrieve the similarity\\nscore between them okay so if we pass\\nboth text and images through this\\nprocessor we'll pass both into our\\ninputs here and then we just calculate\\nthe or we extract the logits for each\\nimage and the item just converts that\\ninto a array of values or single value\\nand then here we have those scores so\\nwhat we're doing here is creating the\\nwhat I earlier called like the relevance\\nmap or localization map throughout the\\nwhole image so for every window that we\\ngo through we're adding this score to\\nevery single patch or little patch\\nwithin that window and what we're going\\nto do or what we're going to find when\\nwe do that is that some patches will\\nnaturally have a high score than others\\nbecause they are viewed more times right\\nso if you think about the top left patch\\nin the image that's only going to be\\nviewed once whereas patches in the\\nmiddle are going to be viewed many times\\nbecause we'll have a sliding window\\ngoing over there multiple times so what\\nwe also need to do is identify the\\nnumber of runs that we perform or number\\nof calculations that we perform within\\neach one of those patches the reason we\\ndo that is so that we can take the\\naverage for each score based on the\\nnumber of times that score has been\\ncapital related because here we're\\ntaking the total Awards scores and then\\nwe'll just take the average like so now\\nthe scores tensor is going to have a\\nvery smooth gradient of values from zero\\ncompletely irrelevant to one now if you\\nconsider that we've been going over\\nthese scores multiple times it means\\nthat the object of interest is kind of\\nlike faded out of the window like over\\nmultiple steps so that means that the\\nsimilarities score quite gradually faves\\nout as you go away from the object which\\nmeans that you don't really get very\\ngood localization if you use these\\nscores directly so what you what we need\\nto do is actually clip the lower scores\\ndown to zero so to do that what it is\\ncalculate the average of scores across\\nthe whole image we subtract that average\\nfrom the current scores what that will\\ndo is push 50 of the scores below zero\\nand then we click those scores so\\nanything below zero becomes zero and we\\ncan do this multiple times okay one time\\nis usually enough but you can do it\\nmultiple times to increase that effect\\nof making the edge of this detected or\\nlocalized area better defined and then\\nafter you've done that what we need to\\ndo is normalize those scores okay so we\\nmight have to do this a few times but\\neverything's probably going to be within\\nthe range of like zero to 0.5 or 0 to\\n0.2 so then we normalize those scores\\nbring them back within the range of zero\\nto one now to apply these scores to the\\npatches we need to align their tenses\\nbecause right now that they are not\\naligned okay for the scores we just we\\nhave like 20 by 13 tensor but for the\\npatches we have the the batch Dimension\\nthere we have the 20 by 13 which we do\\nwant but then we have the three color\\nchannels and the two five six for each\\num set of pixels within each patch so we\\nneed to adjust that a little bit so we\\nneed to First remove the batch Dimension\\nwe do that by squeezing out the zero\\nDimension which is a batch Dimension and\\nthen we permute the different dimensions\\nit's actually just moving them around in\\nour patches in order to align them\\nbetter with the score tensor dimensions\\nand then all we do is multiply the\\npatches by those scores that's pretty\\nstraightforward\\nthen we have to mute them again because\\nif we want to visualize everything needs\\nto be within a certain shape in order\\nfor us to visualize our matplotlib\\nso we come down and the first thing you\\ndo is just get y next here so Y and X\\nare the the patches\\nsee here this is y so the height of the\\nimage in patches and then 13 which is\\nthe width of the image in patches and we\\ncome down here and we can plot this okay\\nand we get this it's pretty nice visual\\nlocalizes the The Fluffy Cuts within\\nthat image now what's really interesting\\nis if we just search for a cat we\\nactually get a slightly different\\nlocalization because here you can see\\nit's kind of focusing a lot on the\\nfluffy part of the cat so if we just\\nsearch for a cat it would actually focus\\nmore on the head so we can really add\\nnuanced information to these prompts and\\nget a pretty nuanced response back\\nnow we can do the same for butterfly so\\nwe'll just throw all that code together\\nthis is just what we've done before we\\ninitialize scores and runs and we go\\nprocess all of that the only thing we\\nchange here is the prompt we change it\\nto a butterfly and if we go down and\\nwe're going to go down and down and\\nvisualize that we'll get this okay so\\nagain that's that's pretty cool we can\\nsee that it is identifying where in the\\nimage that butterfly actually is so that\\nis the object localization set\\nnow I want to have a look at object\\ndetection which is essentially just\\ntaking the object localization and and\\nwrapping some more code around it\\nin order to look at these multiple\\nobjects rather than just one but to do\\nthat we can't really visualize in the\\nsame way that we've done here we're\\ngoing to need a different type of\\nvisualization and that's where we have\\nthe bounding boxes so let's take a look\\nat how we would do that so using the I\\nthink the butterfly examples of\\nbutterfly scores that we just calculated\\nwe're going to look at where those\\nscores are higher than 0.5 now you can\\nadjust this threshold based on you know\\nwhat you find works best so we do this\\nand what we'll get is a array of true\\nand false values as to where the score\\nwas higher than 0.5 and not\\nand then we detect where the non-zero\\nvalues are in that array and what we do\\nis get a load of X and Y values here\\nso position three two we know that there\\nis a score that is higher than 0.5 and\\nwe get three and two here so three is\\nthe row of the non-zero value and 2 is a\\ncolumn of the non-zero value so at row\\nposition three and column two we know\\nthat there is a non-zero value or a\\nvalue or score that's higher than 0.5\\nour threshold\\nand put all that together we'll get\\nsomething looks kind of like this so we\\nalready we kind of see that\\nlocalization visual that we we just\\ncreated\\nand what we want to do is identify the\\nbounding box that's just kind of\\nsurrounding those values okay so we know\\nin terms of like a coordinate system we\\nwant one and three and four and ten to\\nbe included within that so what we do is\\nfind the corners from the detection\\narray or or set of\\ncoordinates that we got before from NP\\nnon-zero and what we do is we just take\\nthe minimum X and Y values and maximum X\\nand Y values and that will give us the\\ncorners of the box\\nthat's pretty simple to to calculate now\\nwhen we get the maximum value what we\\nwant to do is because we basically we\\nget in the position of the patch and the\\nposition of each patch we're essentially\\nidentifying the top left corner of each\\npatch so when we're looking at the\\nmaximum value we actually want not the\\nsolved patch but the end of the patch\\nokay so that's why we add that plus one\\nhere in order to get that\\nthe same for the x max value as well so\\nthat gives us a corner coordinates and\\nthen what we do is multiply those Corner\\ncoordinates by the patch size it's 256\\npixels and then we have the pixel\\npositions of each one of those Corners\\nbecause before we had the patch\\ncoordinates now we have the pixel\\ncoordinates which we can map directly\\nonto the original image so we can see\\nthe minimum values here so we have for x\\nand y two five six and a seven six eight\\nand what we want to do because we're\\ngoing to be using matplotlib patches and\\nmatplotlip patches expects the top left\\ncorner coordinates and the width and\\nheight of the bounding box that you want\\nto create so we calculate the width and\\nheight and that's pretty simple it's\\njust y Max minus y Min and X knives\\nminus X min\\nlook at these\\nand what we can do now is take the image\\nwe have to\\nreshape it a little bit so we have to\\nmove the three color channels Dimension\\nfrom the zeroth dimension to the final\\nDimension so we just do that here move\\naxis\\nand now we can plot that image okay so\\nwe showed that image with matplotlib\\nand then we create a rectangle patch\\nthis is our bounding box okay so we pass\\nX Min and Y Min that's the top left\\ncorner and then we also pass the width\\nand height of what the boundary box\\nshould be\\nand if we come down we get this visual\\nokay so that's our bounding box\\nvisualization and with that it's not\\nmuch further to create our object\\ndetection so let's have a look at how we\\ndo that now the logic for this is pretty\\nmuch just a loop over what we've already\\ndone so I've put together a load of\\nfunctions here which is essentially just\\nwhat we've already gone through getting\\npatches getting the the scores getting\\nthe the box and then the one thing that\\nis new here is this detect function okay\\nso we have detect that's going to get\\nthe the patches so it's going to take an\\nimage and it's going to split into those\\npatches that we created we're going to\\nconvert the image into a format for\\ndisplaying with matplotlib we did that\\nbefore and we also initialized that plot\\nand add our image to that plot and then\\nwhat we do is we have for Loop and this\\nfor Loop goes through the image\\nlocalization steps and bounding box\\nsteps that we just went through\\njust multiple times okay so we have\\nmultiple primes and we want to do it\\nmultiple times so we calculate our\\nsimilarity scores based on a specific\\nprompt\\nfor all of our image Patches from that\\nwe get our our scores in that patch\\ntensor format that we saw before\\nand then what we do is we want to get\\nthe box based on a particular threshold\\nso 0.5 like we used before you can see\\nover there we have our patch size we\\njust need to pass to that for a\\ncalculation of the or for the conversion\\nand we have our patch size which we\\npassed to that for the conversion from\\npatch\\npixel from patch coordinates to pixel\\ncoordinates now we also have our scores\\nand that will return the minimum X and Y\\ncoordinates and also width and height of\\nthe box\\nwe create the bounding box\\nnow we add that to the axis okay so now\\nlet's visualize all this see what we get\\nso here I've used a slightly smaller\\nwindow size before using six just to\\npoint out that you can change it and\\ndepending on your image it may be better\\nto use a smaller or larger window\\nand you can see so what we're doing here\\nwe've got a cat and a butterfly and you\\ncan see that we get we get a butterfly\\nhere and we get the cat here okay it's\\npretty cool and like I said with clip we\\ncan apply this object detection without\\nfine tuning all we need to do is change\\nthese prompts here\\nokay so it's it's really straightforward\\nto modify this and move it to a new\\ndomain okay so that's it for this\\nwalkthrough of object localization and\\nobject detection with clip as I said I\\nthink zero shot object localization\\ndetection and even classification opens\\nthe doors to a lot of projects and use\\ncases that were just not accessible\\nbefore because time and capital\\nconstraints and now we can just use clip\\nand get pretty impressive results very\\nquickly all it requires is a bit of code\\nchanging here and there now I think clip\\nis one part of a trend in multimodality\\nthat is kind of creating a more\\naccessible ml that is less brittle like\\nmodels were in the past that required a\\nlot of fine tuning just to adapt to a\\nslightly different domain and just more\\ngenerally applicable which I thing is\\nreally exciting and I'm I'm it's really\\ncool to see this sort of thing actually\\nbeing used and to actually use it and\\njust see how easy it is to use clip for\\nso many different use cases and it work\\nlike incredibly easily so that's it for\\nthis video I hope it has been useful\\nso thank you very much for watching and\\nI will see you again in the next one bye\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`llama-index-embeddings-huggingface` package not found, please run `pip install llama-index-embeddings-huggingface`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:95\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     96\u001b[0m         HuggingFaceEmbedding,\n\u001b[1;32m     97\u001b[0m     )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     splits \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.embeddings.huggingface'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m ChromaVectorStore(chroma_collection \u001b[38;5;241m=\u001b[39m chroma_collection)\n\u001b[1;32m     10\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m---> 12\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n\u001b[1;32m     19\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy should we use OpenAI Clip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     transformations,\n\u001b[1;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:69\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     85\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llamaindex/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:114\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m    110\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbedding(\n\u001b[1;32m    111\u001b[0m             model_name\u001b[38;5;241m=\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-huggingface` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-huggingface`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LCEmbeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, LCEmbeddings):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: `llama-index-embeddings-huggingface` package not found, please run `pip install llama-index-embeddings-huggingface`"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_youtube\")\n",
    "chroma_collection = db.get_or_create_collection(\"youtube_chroma\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection = chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model = 'local'\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Why should we use OpenAI Clip\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
